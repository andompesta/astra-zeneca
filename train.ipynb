{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34505f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb47ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from torch import nn, Tensor, optim\n",
    "from typing import Optional\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "from src.datapipe import WikiDataset\n",
    "from src.utils.common import PAD\n",
    "from src.utils.training import train_fn, eval_fn\n",
    "from src.modules.graph_encoder import GraphEncoder\n",
    "from src.modules.seq_decoder import DecoderRNN\n",
    "from src.modules.graph_seq import GraphSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606c2205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandompesta\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ando_cavallari/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"fd8e6949c75375b623a566795f8460842fee1e14\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab487d48",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "*Goal* overfit a single batch to verify code correctnes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5c8115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"astrazeneca\"\n",
    "experiment_name = \"single-batch\"\n",
    "\n",
    "trials = [\n",
    "    # trial setup\n",
    "    dict(\n",
    "        job_type=\"train\",\n",
    "        project=project,\n",
    "        group=experiment_name,\n",
    "        notes=\"test training pipeline with a single batch on simple model\",\n",
    "        config=dict(\n",
    "            dataset_base_path=\"data/wiki\",\n",
    "            dataset_name=\"dev\",\n",
    "            vocab_path=\"data/wiki/entity_2_id.bin\",\n",
    "            batch_size=5,\n",
    "            learning_rate=0.003,\n",
    "            device=\"cuda\",\n",
    "            accumulation_steps=1,\n",
    "            max_grad_norm=20.,\n",
    "            epochs=500,\n",
    "            steps_per_epoch=5,\n",
    "            pad_idx=0,\n",
    "            emb_dim=6,\n",
    "            graph_conv_layers=1,\n",
    "            rnn_decoder_layers=1,\n",
    "            rnn_dropout=0.,\n",
    "        ),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c00d8644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ando_cavallari/astra-zeneca/wandb/run-20230202_164332-smwvwuk6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/andompesta/astrazeneca/runs/smwvwuk6\" target=\"_blank\">scintillating-tiger-14</a></strong> to <a href=\"https://wandb.ai/andompesta/astrazeneca\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/andompesta/astrazeneca\" target=\"_blank\">https://wandb.ai/andompesta/astrazeneca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/andompesta/astrazeneca/runs/smwvwuk6\" target=\"_blank\">https://wandb.ai/andompesta/astrazeneca/runs/smwvwuk6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\tacc:0.12941176470588237 \t loss:4.322681713104248\n",
      "epoch:1\tacc:0.17647058823529413 \t loss:4.307934951782227\n",
      "epoch:2\tacc:0.17647058823529413 \t loss:4.28772554397583\n",
      "epoch:3\tacc:0.17647058823529413 \t loss:4.258470153808593\n",
      "epoch:4\tacc:0.14705882352941177 \t loss:4.214743137359619\n",
      "epoch:5\tacc:0.14705882352941177 \t loss:4.1482008934021\n",
      "epoch:6\tacc:0.17058823529411765 \t loss:4.04694504737854\n",
      "epoch:7\tacc:0.17647058823529413 \t loss:3.8974583625793455\n",
      "epoch:8\tacc:0.13529411764705881 \t loss:3.6893190860748293\n",
      "epoch:9\tacc:0.1588235294117647 \t loss:3.415463399887085\n",
      "epoch:10\tacc:0.10588235294117647 \t loss:3.066850519180298\n",
      "epoch:11\tacc:0.07647058823529412 \t loss:2.6263970375061034\n",
      "epoch:12\tacc:0.03529411764705882 \t loss:2.0766141414642334\n",
      "epoch:13\tacc:0.029411764705882353 \t loss:1.4917224884033202\n",
      "epoch:14\tacc:0.2411764705882353 \t loss:1.1506112813949585\n",
      "epoch:15\tacc:0.29411764705882354 \t loss:1.0390278100967407\n",
      "epoch:16\tacc:0.38823529411764707 \t loss:1.0054585456848144\n",
      "epoch:17\tacc:0.3 \t loss:1.0007668375968932\n",
      "epoch:18\tacc:0.28823529411764703 \t loss:0.9850984215736389\n",
      "epoch:19\tacc:0.18823529411764706 \t loss:0.9621477365493775\n",
      "epoch:20\tacc:0.2823529411764706 \t loss:0.9431149005889893\n",
      "epoch:21\tacc:0.4176470588235294 \t loss:0.9272737741470337\n",
      "epoch:22\tacc:0.4117647058823529 \t loss:0.9103006720542908\n",
      "epoch:23\tacc:0.4176470588235294 \t loss:0.892572021484375\n",
      "epoch:24\tacc:0.4411764705882353 \t loss:0.8772876977920532\n",
      "epoch:25\tacc:0.4411764705882353 \t loss:0.8627934217453003\n",
      "epoch:26\tacc:0.4411764705882353 \t loss:0.8487787365913391\n",
      "epoch:27\tacc:0.4411764705882353 \t loss:0.8349360585212707\n",
      "epoch:28\tacc:0.4411764705882353 \t loss:0.8205031394958496\n",
      "epoch:29\tacc:0.4411764705882353 \t loss:0.8050450801849365\n",
      "epoch:30\tacc:0.4411764705882353 \t loss:0.7887135982513428\n",
      "epoch:31\tacc:0.4411764705882353 \t loss:0.7721582412719726\n",
      "epoch:32\tacc:0.4411764705882353 \t loss:0.7558908820152282\n",
      "epoch:33\tacc:0.4411764705882353 \t loss:0.7398905992507935\n",
      "epoch:34\tacc:0.4411764705882353 \t loss:0.7239272952079773\n",
      "epoch:35\tacc:0.4411764705882353 \t loss:0.7079826831817627\n",
      "epoch:36\tacc:0.4411764705882353 \t loss:0.6922563672065735\n",
      "epoch:37\tacc:0.4411764705882353 \t loss:0.6769596815109253\n",
      "epoch:38\tacc:0.4411764705882353 \t loss:0.6621447086334229\n",
      "epoch:39\tacc:0.4470588235294118 \t loss:0.6473162055015564\n",
      "epoch:40\tacc:0.48823529411764705 \t loss:0.6321980357170105\n",
      "epoch:41\tacc:0.5 \t loss:0.6191097974777222\n",
      "epoch:42\tacc:0.5117647058823529 \t loss:0.6060303807258606\n",
      "epoch:43\tacc:0.5294117647058824 \t loss:0.5945432305335998\n",
      "epoch:44\tacc:0.5823529411764706 \t loss:0.5840918779373169\n",
      "epoch:45\tacc:0.5882352941176471 \t loss:0.5746848702430725\n",
      "epoch:46\tacc:0.5882352941176471 \t loss:0.5660482287406922\n",
      "epoch:47\tacc:0.5882352941176471 \t loss:0.5581007242202759\n",
      "epoch:48\tacc:0.5882352941176471 \t loss:0.5504848599433899\n",
      "epoch:49\tacc:0.5882352941176471 \t loss:0.5429217457771301\n",
      "epoch:50\tacc:0.5823529411764706 \t loss:0.5355921983718872\n",
      "epoch:51\tacc:0.5823529411764706 \t loss:0.5278995513916016\n",
      "epoch:52\tacc:0.5823529411764706 \t loss:0.5199184536933898\n",
      "epoch:53\tacc:0.5764705882352941 \t loss:0.5118115544319153\n",
      "epoch:54\tacc:0.5823529411764706 \t loss:0.5030579328536987\n",
      "epoch:55\tacc:0.5705882352941176 \t loss:0.4939348757266998\n",
      "epoch:56\tacc:0.5941176470588235 \t loss:0.48372396230697634\n",
      "epoch:57\tacc:0.6058823529411764 \t loss:0.47305771708488464\n",
      "epoch:58\tacc:0.6 \t loss:0.46251216530799866\n",
      "epoch:59\tacc:0.6058823529411764 \t loss:0.4514990210533142\n",
      "epoch:60\tacc:0.6176470588235294 \t loss:0.44008315205574033\n",
      "epoch:61\tacc:0.6235294117647059 \t loss:0.42942371368408205\n",
      "epoch:62\tacc:0.6294117647058823 \t loss:0.41926629543304444\n",
      "epoch:63\tacc:0.6470588235294118 \t loss:0.41039918065071107\n",
      "epoch:64\tacc:0.6529411764705882 \t loss:0.40234798192977905\n",
      "epoch:65\tacc:0.6588235294117647 \t loss:0.3949612438678741\n",
      "epoch:66\tacc:0.6705882352941176 \t loss:0.38826342225074767\n",
      "epoch:67\tacc:0.6647058823529411 \t loss:0.3821011781692505\n",
      "epoch:68\tacc:0.6705882352941176 \t loss:0.37691107392311096\n",
      "epoch:69\tacc:0.6823529411764706 \t loss:0.37337952852249146\n",
      "epoch:70\tacc:0.6588235294117647 \t loss:0.37787981033325196\n",
      "epoch:71\tacc:0.6647058823529411 \t loss:0.37295695543289187\n",
      "epoch:72\tacc:0.6882352941176471 \t loss:0.36038491129875183\n",
      "epoch:73\tacc:0.7 \t loss:0.35338011384010315\n",
      "epoch:74\tacc:0.6941176470588235 \t loss:0.3444971740245819\n",
      "epoch:75\tacc:0.7529411764705882 \t loss:0.33823809027671814\n",
      "epoch:76\tacc:0.7294117647058823 \t loss:0.33397729992866515\n",
      "epoch:77\tacc:0.7411764705882353 \t loss:0.33354241251945493\n",
      "epoch:78\tacc:0.7529411764705882 \t loss:0.3261716842651367\n",
      "epoch:79\tacc:0.7470588235294118 \t loss:0.328987193107605\n",
      "epoch:80\tacc:0.7352941176470589 \t loss:0.3242108583450317\n",
      "epoch:81\tacc:0.7588235294117647 \t loss:0.30550460815429686\n",
      "epoch:82\tacc:0.7647058823529411 \t loss:0.30200673937797545\n",
      "epoch:83\tacc:0.7941176470588235 \t loss:0.28775752186775205\n",
      "epoch:84\tacc:0.7470588235294118 \t loss:0.30437851548194883\n",
      "epoch:85\tacc:0.7470588235294118 \t loss:0.28881061673164365\n",
      "epoch:86\tacc:0.7529411764705882 \t loss:0.2852619767189026\n",
      "epoch:87\tacc:0.7823529411764706 \t loss:0.2784425675868988\n",
      "epoch:88\tacc:0.8 \t loss:0.26712180972099303\n",
      "epoch:89\tacc:0.8117647058823529 \t loss:0.2593745648860931\n",
      "epoch:90\tacc:0.8117647058823529 \t loss:0.24642982184886933\n",
      "epoch:91\tacc:0.8058823529411765 \t loss:0.25206991732120515\n",
      "epoch:92\tacc:0.788235294117647 \t loss:0.24404760897159578\n",
      "epoch:93\tacc:0.8 \t loss:0.23762530386447905\n",
      "epoch:94\tacc:0.8176470588235294 \t loss:0.23247747719287873\n",
      "epoch:95\tacc:0.8294117647058824 \t loss:0.22350988984107972\n",
      "epoch:96\tacc:0.8352941176470589 \t loss:0.21748288869857788\n",
      "epoch:97\tacc:0.8411764705882353 \t loss:0.21300365626811982\n",
      "epoch:98\tacc:0.7823529411764706 \t loss:0.25742395520210265\n",
      "epoch:99\tacc:0.8294117647058824 \t loss:0.22646909654140474\n",
      "epoch:100\tacc:0.8352941176470589 \t loss:0.21352449357509612\n",
      "epoch:101\tacc:0.8117647058823529 \t loss:0.22579551935195924\n",
      "epoch:102\tacc:0.7823529411764706 \t loss:0.22164050340652466\n",
      "epoch:103\tacc:0.8235294117647058 \t loss:0.21475594341754914\n",
      "epoch:104\tacc:0.8117647058823529 \t loss:0.22329062223434448\n",
      "epoch:105\tacc:0.8705882352941177 \t loss:0.20096812546253204\n",
      "epoch:106\tacc:0.8705882352941177 \t loss:0.19340237081050873\n",
      "epoch:107\tacc:0.8705882352941177 \t loss:0.1863173484802246\n",
      "epoch:108\tacc:0.8823529411764706 \t loss:0.1799307197332382\n",
      "epoch:109\tacc:0.9058823529411765 \t loss:0.17551062107086182\n",
      "epoch:110\tacc:0.888235294117647 \t loss:0.1786746382713318\n",
      "epoch:111\tacc:0.888235294117647 \t loss:0.17291173040866853\n",
      "epoch:112\tacc:0.8823529411764706 \t loss:0.16954457461833955\n",
      "epoch:113\tacc:0.8823529411764706 \t loss:0.16758459508419038\n",
      "epoch:114\tacc:0.8764705882352941 \t loss:0.16207291781902314\n",
      "epoch:115\tacc:0.888235294117647 \t loss:0.1577679455280304\n",
      "epoch:116\tacc:0.8764705882352941 \t loss:0.16048559844493865\n",
      "epoch:117\tacc:0.8588235294117647 \t loss:0.1899491399526596\n",
      "epoch:118\tacc:0.7529411764705882 \t loss:0.32998768985271454\n",
      "epoch:119\tacc:0.8411764705882353 \t loss:0.19826892912387847\n",
      "epoch:120\tacc:0.8647058823529412 \t loss:0.1867743194103241\n",
      "epoch:121\tacc:0.8823529411764706 \t loss:0.16954387128353118\n",
      "epoch:122\tacc:0.8941176470588236 \t loss:0.15883446633815765\n",
      "epoch:123\tacc:0.888235294117647 \t loss:0.1612097978591919\n",
      "epoch:124\tacc:0.9058823529411765 \t loss:0.15033300817012787\n",
      "epoch:125\tacc:0.9411764705882353 \t loss:0.1446525424718857\n",
      "epoch:126\tacc:0.8823529411764706 \t loss:0.19163078367710112\n",
      "epoch:127\tacc:0.9058823529411765 \t loss:0.15799545347690583\n",
      "epoch:128\tacc:0.9352941176470588 \t loss:0.14813323318958282\n",
      "epoch:129\tacc:0.9176470588235294 \t loss:0.13767671585083008\n",
      "epoch:130\tacc:0.9352941176470588 \t loss:0.13541938364505768\n",
      "epoch:131\tacc:0.9 \t loss:0.15496759712696076\n",
      "epoch:132\tacc:0.8941176470588236 \t loss:0.15219362080097198\n",
      "epoch:133\tacc:0.8823529411764706 \t loss:0.14915914237499237\n",
      "epoch:134\tacc:0.9294117647058824 \t loss:0.13211632072925567\n",
      "epoch:135\tacc:0.9352941176470588 \t loss:0.12456071674823761\n",
      "epoch:136\tacc:0.9235294117647059 \t loss:0.12505655139684677\n",
      "epoch:137\tacc:0.9411764705882353 \t loss:0.1197103127837181\n",
      "epoch:138\tacc:0.9 \t loss:0.15031027644872666\n",
      "epoch:139\tacc:0.888235294117647 \t loss:0.17992187589406966\n",
      "epoch:140\tacc:0.8470588235294118 \t loss:0.20419859439134597\n",
      "epoch:141\tacc:0.8529411764705882 \t loss:0.1841052532196045\n",
      "epoch:142\tacc:0.8352941176470589 \t loss:0.18028267323970795\n",
      "epoch:143\tacc:0.9 \t loss:0.141442009806633\n",
      "epoch:144\tacc:0.9 \t loss:0.12812287956476212\n",
      "epoch:145\tacc:0.9117647058823529 \t loss:0.15675620138645172\n",
      "epoch:146\tacc:0.9352941176470588 \t loss:0.12064854353666306\n",
      "epoch:147\tacc:0.9235294117647059 \t loss:0.12622282952070235\n",
      "epoch:148\tacc:0.9235294117647059 \t loss:0.1157821387052536\n",
      "epoch:149\tacc:0.9470588235294117 \t loss:0.11242763698101044\n",
      "epoch:150\tacc:0.9470588235294117 \t loss:0.10567649155855179\n",
      "epoch:151\tacc:0.9470588235294117 \t loss:0.10273920893669128\n",
      "epoch:152\tacc:0.9470588235294117 \t loss:0.10111475586891175\n",
      "epoch:153\tacc:0.9470588235294117 \t loss:0.09880657494068146\n",
      "epoch:154\tacc:0.9470588235294117 \t loss:0.09632724672555923\n",
      "epoch:155\tacc:0.9588235294117647 \t loss:0.09645571410655976\n",
      "epoch:156\tacc:0.9235294117647059 \t loss:0.13852245062589646\n",
      "epoch:157\tacc:0.9352941176470588 \t loss:0.11623740345239639\n",
      "epoch:158\tacc:0.9352941176470588 \t loss:0.10779261291027069\n",
      "epoch:159\tacc:0.9235294117647059 \t loss:0.10418995916843414\n",
      "epoch:160\tacc:0.9176470588235294 \t loss:0.10822223424911499\n",
      "epoch:161\tacc:0.9470588235294117 \t loss:0.10508767515420914\n",
      "epoch:162\tacc:0.9470588235294117 \t loss:0.10298013538122178\n",
      "epoch:163\tacc:0.9588235294117647 \t loss:0.10188557207584381\n",
      "epoch:164\tacc:0.9529411764705882 \t loss:0.09995137751102448\n",
      "epoch:165\tacc:0.9764705882352941 \t loss:0.09740807265043258\n",
      "epoch:166\tacc:0.9529411764705882 \t loss:0.10125983208417892\n",
      "epoch:167\tacc:0.9588235294117647 \t loss:0.09952226132154465\n",
      "epoch:168\tacc:0.9470588235294117 \t loss:0.09596204310655594\n",
      "epoch:169\tacc:0.9647058823529412 \t loss:0.0951602965593338\n",
      "epoch:170\tacc:0.9588235294117647 \t loss:0.09796022474765778\n",
      "epoch:171\tacc:0.9529411764705882 \t loss:0.0990604281425476\n",
      "epoch:172\tacc:0.9529411764705882 \t loss:0.09708957374095917\n",
      "epoch:173\tacc:0.9588235294117647 \t loss:0.09371835887432098\n",
      "epoch:174\tacc:0.9588235294117647 \t loss:0.09726456850767136\n",
      "epoch:175\tacc:0.9529411764705882 \t loss:0.0961480900645256\n",
      "epoch:176\tacc:0.9411764705882353 \t loss:0.09464743584394456\n",
      "epoch:177\tacc:0.9352941176470588 \t loss:0.10295165926218033\n",
      "epoch:178\tacc:0.9705882352941176 \t loss:0.09527826458215713\n",
      "epoch:179\tacc:0.9529411764705882 \t loss:0.10532654523849487\n",
      "epoch:180\tacc:0.9235294117647059 \t loss:0.12519926726818084\n",
      "epoch:181\tacc:0.9294117647058824 \t loss:0.10485840290784836\n",
      "epoch:182\tacc:0.9294117647058824 \t loss:0.09139972627162933\n",
      "epoch:183\tacc:0.9235294117647059 \t loss:0.1091166839003563\n",
      "epoch:184\tacc:0.9411764705882353 \t loss:0.09273931682109833\n",
      "epoch:185\tacc:0.9470588235294117 \t loss:0.08390298634767532\n",
      "epoch:186\tacc:0.9176470588235294 \t loss:0.09700445830821991\n",
      "epoch:187\tacc:0.9470588235294117 \t loss:0.09886782616376877\n",
      "epoch:188\tacc:0.9529411764705882 \t loss:0.0788803294301033\n",
      "epoch:189\tacc:0.9470588235294117 \t loss:0.0978406623005867\n",
      "epoch:190\tacc:0.9294117647058824 \t loss:0.08399706184864045\n",
      "epoch:191\tacc:0.9588235294117647 \t loss:0.07318969666957856\n",
      "epoch:192\tacc:0.9294117647058824 \t loss:0.09469003677368164\n",
      "epoch:193\tacc:0.9588235294117647 \t loss:0.07866269648075104\n",
      "epoch:194\tacc:0.9352941176470588 \t loss:0.07760011553764343\n",
      "epoch:195\tacc:0.9647058823529412 \t loss:0.07836180329322814\n",
      "epoch:196\tacc:0.9647058823529412 \t loss:0.07498062551021575\n",
      "epoch:197\tacc:0.9411764705882353 \t loss:0.07882597148418427\n",
      "epoch:198\tacc:0.9647058823529412 \t loss:0.07579012513160706\n",
      "epoch:199\tacc:0.9529411764705882 \t loss:0.07329126596450805\n",
      "epoch:200\tacc:0.9235294117647059 \t loss:0.10221544206142426\n",
      "epoch:201\tacc:0.9529411764705882 \t loss:0.07376371920108796\n",
      "epoch:202\tacc:0.9529411764705882 \t loss:0.07199878245592117\n",
      "epoch:203\tacc:0.9705882352941176 \t loss:0.06760347709059715\n",
      "epoch:204\tacc:0.9352941176470588 \t loss:0.11233514696359634\n",
      "epoch:205\tacc:0.9058823529411765 \t loss:0.1289849877357483\n",
      "epoch:206\tacc:0.8647058823529412 \t loss:0.15693266689777374\n",
      "epoch:207\tacc:0.8705882352941177 \t loss:0.1334623619914055\n",
      "epoch:208\tacc:0.9235294117647059 \t loss:0.0958435669541359\n",
      "epoch:209\tacc:0.9058823529411765 \t loss:0.11947401463985444\n",
      "epoch:210\tacc:0.9411764705882353 \t loss:0.07764451950788498\n",
      "epoch:211\tacc:0.9588235294117647 \t loss:0.06925604343414307\n",
      "epoch:212\tacc:0.9294117647058824 \t loss:0.09847968071699142\n",
      "epoch:213\tacc:0.8823529411764706 \t loss:0.16243053674697877\n",
      "epoch:214\tacc:0.8823529411764706 \t loss:0.13914129883050919\n",
      "epoch:215\tacc:0.9176470588235294 \t loss:0.10158666968345642\n",
      "epoch:216\tacc:0.9235294117647059 \t loss:0.08788602501153946\n",
      "epoch:217\tacc:0.9235294117647059 \t loss:0.10946314334869385\n",
      "epoch:218\tacc:0.9588235294117647 \t loss:0.06525246724486351\n",
      "epoch:219\tacc:0.9647058823529412 \t loss:0.060947779566049576\n",
      "epoch:220\tacc:0.9647058823529412 \t loss:0.05830983519554138\n",
      "epoch:221\tacc:0.9764705882352941 \t loss:0.05616500973701477\n",
      "epoch:222\tacc:0.9764705882352941 \t loss:0.05544042289257049\n",
      "epoch:223\tacc:0.9705882352941176 \t loss:0.05366233512759209\n",
      "epoch:224\tacc:0.9705882352941176 \t loss:0.05235739722847939\n",
      "epoch:225\tacc:0.9705882352941176 \t loss:0.05170227587223053\n",
      "epoch:226\tacc:0.9705882352941176 \t loss:0.050264942646026614\n",
      "epoch:227\tacc:0.9705882352941176 \t loss:0.049628615379333496\n",
      "epoch:228\tacc:0.9705882352941176 \t loss:0.05143696665763855\n",
      "epoch:229\tacc:0.9588235294117647 \t loss:0.05159901380538941\n",
      "epoch:230\tacc:0.9411764705882353 \t loss:0.06589133441448211\n",
      "epoch:231\tacc:0.9470588235294117 \t loss:0.06519482284784317\n",
      "epoch:232\tacc:0.9117647058823529 \t loss:0.11364282369613647\n",
      "epoch:233\tacc:0.8823529411764706 \t loss:0.14358285516500474\n",
      "epoch:234\tacc:0.9 \t loss:0.1245463415980339\n",
      "epoch:235\tacc:0.9235294117647059 \t loss:0.0979270190000534\n",
      "epoch:236\tacc:0.9235294117647059 \t loss:0.10158023387193679\n",
      "epoch:237\tacc:0.888235294117647 \t loss:0.13481575697660447\n",
      "epoch:238\tacc:0.9176470588235294 \t loss:0.09057271331548691\n",
      "epoch:239\tacc:0.9294117647058824 \t loss:0.07143337205052376\n",
      "epoch:240\tacc:0.9529411764705882 \t loss:0.06265898495912552\n",
      "epoch:241\tacc:0.9470588235294117 \t loss:0.08336428850889206\n",
      "epoch:242\tacc:0.9176470588235294 \t loss:0.09173642471432686\n",
      "epoch:243\tacc:0.9294117647058824 \t loss:0.07644384354352951\n",
      "epoch:244\tacc:0.9352941176470588 \t loss:0.09066246151924133\n",
      "epoch:245\tacc:0.9647058823529412 \t loss:0.06580170392990112\n",
      "epoch:246\tacc:0.9647058823529412 \t loss:0.05721122846007347\n",
      "epoch:247\tacc:0.9529411764705882 \t loss:0.06546810120344163\n",
      "epoch:248\tacc:0.8941176470588236 \t loss:0.12280454337596894\n",
      "epoch:249\tacc:0.9117647058823529 \t loss:0.10395705997943878\n",
      "epoch:250\tacc:0.9117647058823529 \t loss:0.1097053661942482\n",
      "epoch:251\tacc:0.9058823529411765 \t loss:0.12207642197608948\n",
      "epoch:252\tacc:0.9176470588235294 \t loss:0.09802982658147812\n",
      "epoch:253\tacc:0.9117647058823529 \t loss:0.11006353050470352\n",
      "epoch:254\tacc:0.9529411764705882 \t loss:0.06613969877362251\n",
      "epoch:255\tacc:0.8941176470588236 \t loss:0.13195552304387093\n",
      "epoch:256\tacc:0.9294117647058824 \t loss:0.09967090636491775\n",
      "epoch:257\tacc:0.9294117647058824 \t loss:0.09087133780121803\n",
      "epoch:258\tacc:0.9529411764705882 \t loss:0.06981281712651252\n",
      "epoch:259\tacc:0.9352941176470588 \t loss:0.07164597660303115\n",
      "epoch:260\tacc:0.9294117647058824 \t loss:0.08867707774043083\n",
      "epoch:261\tacc:0.9176470588235294 \t loss:0.10507595241069793\n",
      "epoch:262\tacc:0.9117647058823529 \t loss:0.09940438568592072\n",
      "epoch:263\tacc:0.9647058823529412 \t loss:0.06901594549417496\n",
      "epoch:264\tacc:0.9470588235294117 \t loss:0.056227728724479675\n",
      "epoch:265\tacc:0.9588235294117647 \t loss:0.057462926208972934\n",
      "epoch:266\tacc:0.9352941176470588 \t loss:0.07567281350493431\n",
      "epoch:267\tacc:0.9352941176470588 \t loss:0.07607079073786735\n",
      "epoch:268\tacc:0.9529411764705882 \t loss:0.06023044809699059\n",
      "epoch:269\tacc:0.9470588235294117 \t loss:0.05979565754532814\n",
      "epoch:270\tacc:0.9470588235294117 \t loss:0.07314630523324013\n",
      "epoch:271\tacc:0.9647058823529412 \t loss:0.04688168093562126\n",
      "epoch:272\tacc:0.9705882352941176 \t loss:0.04553134962916374\n",
      "epoch:273\tacc:0.9647058823529412 \t loss:0.048176341503858564\n",
      "epoch:274\tacc:0.9764705882352941 \t loss:0.04303851127624512\n",
      "epoch:275\tacc:0.9705882352941176 \t loss:0.04182153046131134\n",
      "epoch:276\tacc:0.9529411764705882 \t loss:0.08242330849170684\n",
      "epoch:277\tacc:0.9588235294117647 \t loss:0.05533158853650093\n",
      "epoch:278\tacc:0.9470588235294117 \t loss:0.05822942107915878\n",
      "epoch:279\tacc:0.9588235294117647 \t loss:0.05078941285610199\n",
      "epoch:280\tacc:0.9647058823529412 \t loss:0.04824233278632164\n",
      "epoch:281\tacc:0.9823529411764705 \t loss:0.04094296172261238\n",
      "epoch:282\tacc:0.9764705882352941 \t loss:0.049226097762584686\n",
      "epoch:283\tacc:0.9705882352941176 \t loss:0.050909511744976044\n",
      "epoch:284\tacc:0.9647058823529412 \t loss:0.049951179325580596\n",
      "epoch:285\tacc:0.9411764705882353 \t loss:0.062178580462932585\n",
      "epoch:286\tacc:0.9352941176470588 \t loss:0.066513030230999\n",
      "epoch:287\tacc:0.9529411764705882 \t loss:0.05732377171516419\n",
      "epoch:288\tacc:0.9529411764705882 \t loss:0.05143707320094108\n",
      "epoch:289\tacc:0.9588235294117647 \t loss:0.053433388471603394\n",
      "epoch:290\tacc:0.9470588235294117 \t loss:0.05942340344190598\n",
      "epoch:291\tacc:0.9411764705882353 \t loss:0.06284045726060868\n",
      "epoch:292\tacc:0.9235294117647059 \t loss:0.09074017405509949\n",
      "epoch:293\tacc:0.8941176470588236 \t loss:0.10953292846679688\n",
      "epoch:294\tacc:0.888235294117647 \t loss:0.11726004034280776\n",
      "epoch:295\tacc:0.8764705882352941 \t loss:0.13529419749975205\n",
      "epoch:296\tacc:0.8529411764705882 \t loss:0.13930537849664687\n",
      "epoch:297\tacc:0.9235294117647059 \t loss:0.08789078593254089\n",
      "epoch:298\tacc:0.8588235294117647 \t loss:0.12460011094808579\n",
      "epoch:299\tacc:0.8411764705882353 \t loss:0.15424320101737976\n",
      "epoch:300\tacc:0.8588235294117647 \t loss:0.14766906648874284\n",
      "epoch:301\tacc:0.9235294117647059 \t loss:0.09608478844165802\n",
      "epoch:302\tacc:0.8823529411764706 \t loss:0.12656048387289048\n",
      "epoch:303\tacc:0.9294117647058824 \t loss:0.08773811757564545\n",
      "epoch:304\tacc:0.9529411764705882 \t loss:0.06674663573503495\n",
      "epoch:305\tacc:0.9470588235294117 \t loss:0.06139447763562202\n",
      "epoch:306\tacc:0.9647058823529412 \t loss:0.05705481246113777\n",
      "epoch:307\tacc:0.9647058823529412 \t loss:0.05388977751135826\n",
      "epoch:308\tacc:0.9705882352941176 \t loss:0.049517246335744856\n",
      "epoch:309\tacc:0.9705882352941176 \t loss:0.04695724695920944\n",
      "epoch:310\tacc:0.9705882352941176 \t loss:0.04230850040912628\n",
      "epoch:311\tacc:0.9705882352941176 \t loss:0.04201480224728584\n",
      "epoch:312\tacc:0.9705882352941176 \t loss:0.04058710783720017\n",
      "epoch:313\tacc:0.9705882352941176 \t loss:0.03828307092189789\n",
      "epoch:314\tacc:0.9647058823529412 \t loss:0.041819002479314804\n",
      "epoch:315\tacc:0.9764705882352941 \t loss:0.03710962608456612\n",
      "epoch:316\tacc:0.9705882352941176 \t loss:0.035808391869068146\n",
      "epoch:317\tacc:0.9705882352941176 \t loss:0.03507268875837326\n",
      "epoch:318\tacc:0.9764705882352941 \t loss:0.03391944169998169\n",
      "epoch:319\tacc:0.9647058823529412 \t loss:0.04942662492394447\n",
      "epoch:320\tacc:0.9705882352941176 \t loss:0.04994203224778175\n",
      "epoch:321\tacc:0.9647058823529412 \t loss:0.04569494128227234\n",
      "epoch:322\tacc:0.9705882352941176 \t loss:0.05871615186333656\n",
      "epoch:323\tacc:0.9705882352941176 \t loss:0.043883394449949265\n",
      "epoch:324\tacc:0.9705882352941176 \t loss:0.04727192595601082\n",
      "epoch:325\tacc:0.9647058823529412 \t loss:0.051609137654304506\n",
      "epoch:326\tacc:0.9352941176470588 \t loss:0.0659915842115879\n",
      "epoch:327\tacc:0.8941176470588236 \t loss:0.13760626018047334\n",
      "epoch:328\tacc:0.9294117647058824 \t loss:0.07772208005189896\n",
      "epoch:329\tacc:0.9647058823529412 \t loss:0.040305445343255995\n",
      "epoch:330\tacc:0.9764705882352941 \t loss:0.036853141337633136\n",
      "epoch:331\tacc:0.9764705882352941 \t loss:0.035763292014598845\n",
      "epoch:332\tacc:0.9764705882352941 \t loss:0.034798169136047365\n",
      "epoch:333\tacc:0.9647058823529412 \t loss:0.03962491527199745\n",
      "epoch:334\tacc:0.9588235294117647 \t loss:0.04738273620605469\n",
      "epoch:335\tacc:0.9411764705882353 \t loss:0.0586177833378315\n",
      "epoch:336\tacc:0.9647058823529412 \t loss:0.04192193374037743\n",
      "epoch:337\tacc:0.9705882352941176 \t loss:0.04155845567584038\n",
      "epoch:338\tacc:0.9588235294117647 \t loss:0.04166859462857246\n",
      "epoch:339\tacc:0.9705882352941176 \t loss:0.0371118426322937\n",
      "epoch:340\tacc:0.9705882352941176 \t loss:0.036659106612205505\n",
      "epoch:341\tacc:0.9705882352941176 \t loss:0.03459644764661789\n",
      "epoch:342\tacc:0.9705882352941176 \t loss:0.034654108062386516\n",
      "epoch:343\tacc:0.9705882352941176 \t loss:0.03290118910372257\n",
      "epoch:344\tacc:0.9882352941176471 \t loss:0.03125148564577103\n",
      "epoch:345\tacc:0.9647058823529412 \t loss:0.03536343015730381\n",
      "epoch:346\tacc:0.9352941176470588 \t loss:0.07502821832895279\n",
      "epoch:347\tacc:0.9588235294117647 \t loss:0.041646819561719894\n",
      "epoch:348\tacc:0.9764705882352941 \t loss:0.04625217542052269\n",
      "epoch:349\tacc:0.9588235294117647 \t loss:0.047431530803442\n",
      "epoch:350\tacc:0.8941176470588236 \t loss:0.125986398011446\n",
      "epoch:351\tacc:0.9 \t loss:0.09998260736465454\n",
      "epoch:352\tacc:0.9588235294117647 \t loss:0.05179590210318565\n",
      "epoch:353\tacc:0.9411764705882353 \t loss:0.06365251615643501\n",
      "epoch:354\tacc:0.9705882352941176 \t loss:0.04135200381278992\n",
      "epoch:355\tacc:0.9588235294117647 \t loss:0.04144891276955605\n",
      "epoch:356\tacc:0.9529411764705882 \t loss:0.04494996964931488\n",
      "epoch:357\tacc:0.9529411764705882 \t loss:0.06817682087421417\n",
      "epoch:358\tacc:0.9647058823529412 \t loss:0.03912722319364548\n",
      "epoch:359\tacc:0.9588235294117647 \t loss:0.04217027500271797\n",
      "epoch:360\tacc:0.9705882352941176 \t loss:0.03410863056778908\n",
      "epoch:361\tacc:0.9705882352941176 \t loss:0.03272114619612694\n",
      "epoch:362\tacc:0.9705882352941176 \t loss:0.031203360483050346\n",
      "epoch:363\tacc:0.9705882352941176 \t loss:0.030190229788422583\n",
      "epoch:364\tacc:0.9764705882352941 \t loss:0.02913551591336727\n",
      "epoch:365\tacc:0.9705882352941176 \t loss:0.02879195436835289\n",
      "epoch:366\tacc:0.9705882352941176 \t loss:0.028275658190250397\n",
      "epoch:367\tacc:0.9705882352941176 \t loss:0.030384488776326178\n",
      "epoch:368\tacc:0.9647058823529412 \t loss:0.03791077733039856\n",
      "epoch:369\tacc:0.9705882352941176 \t loss:0.03459911420941353\n",
      "epoch:370\tacc:0.9529411764705882 \t loss:0.05639620609581471\n",
      "epoch:371\tacc:0.9 \t loss:0.11485813111066819\n",
      "epoch:372\tacc:0.9235294117647059 \t loss:0.08189841657876969\n",
      "epoch:373\tacc:0.9 \t loss:0.09676868990063667\n",
      "epoch:374\tacc:0.9647058823529412 \t loss:0.054191536456346515\n",
      "epoch:375\tacc:0.9588235294117647 \t loss:0.04039110392332077\n",
      "epoch:376\tacc:0.9647058823529412 \t loss:0.03808598443865776\n",
      "epoch:377\tacc:0.9705882352941176 \t loss:0.031222111731767654\n",
      "epoch:378\tacc:0.9705882352941176 \t loss:0.030623963847756386\n",
      "epoch:379\tacc:0.9705882352941176 \t loss:0.02896491214632988\n",
      "epoch:380\tacc:0.9764705882352941 \t loss:0.02805480547249317\n",
      "epoch:381\tacc:0.9588235294117647 \t loss:0.04731998555362225\n",
      "epoch:382\tacc:0.9588235294117647 \t loss:0.03931680843234062\n",
      "epoch:383\tacc:0.9764705882352941 \t loss:0.031124245002865793\n",
      "epoch:384\tacc:0.9705882352941176 \t loss:0.028835269063711165\n",
      "epoch:385\tacc:0.9764705882352941 \t loss:0.027764981240034105\n",
      "epoch:386\tacc:0.9764705882352941 \t loss:0.027080662548542023\n",
      "epoch:387\tacc:0.9764705882352941 \t loss:0.02725299820303917\n",
      "epoch:388\tacc:0.9705882352941176 \t loss:0.026716363802552223\n",
      "epoch:389\tacc:0.9705882352941176 \t loss:0.02633502297103405\n",
      "epoch:390\tacc:0.9647058823529412 \t loss:0.029468945786356927\n",
      "epoch:391\tacc:0.9411764705882353 \t loss:0.06753156371414662\n",
      "epoch:392\tacc:0.9529411764705882 \t loss:0.038289431110024454\n",
      "epoch:393\tacc:0.9764705882352941 \t loss:0.033880265057086946\n",
      "epoch:394\tacc:0.9764705882352941 \t loss:0.03418014198541641\n",
      "epoch:395\tacc:0.9764705882352941 \t loss:0.03278381563723087\n",
      "epoch:396\tacc:0.9705882352941176 \t loss:0.028816824406385423\n",
      "epoch:397\tacc:0.9764705882352941 \t loss:0.027343498542904854\n",
      "epoch:398\tacc:0.9764705882352941 \t loss:0.026347715780138968\n",
      "epoch:399\tacc:0.9764705882352941 \t loss:0.025755950808525087\n",
      "epoch:400\tacc:0.9764705882352941 \t loss:0.025353987142443656\n",
      "epoch:401\tacc:0.9411764705882353 \t loss:0.06751114167273045\n",
      "epoch:402\tacc:0.9411764705882353 \t loss:0.053329133242368695\n",
      "epoch:403\tacc:0.9764705882352941 \t loss:0.03571438081562519\n",
      "epoch:404\tacc:0.9588235294117647 \t loss:0.04453663527965546\n",
      "epoch:405\tacc:0.9705882352941176 \t loss:0.047761617600917815\n",
      "epoch:406\tacc:0.9470588235294117 \t loss:0.056926752999424936\n",
      "epoch:407\tacc:0.9411764705882353 \t loss:0.0701751608401537\n",
      "epoch:408\tacc:0.9588235294117647 \t loss:0.03687094524502754\n",
      "epoch:409\tacc:0.9588235294117647 \t loss:0.03861171081662178\n",
      "epoch:410\tacc:0.9705882352941176 \t loss:0.0288525253534317\n",
      "epoch:411\tacc:0.9705882352941176 \t loss:0.02698567658662796\n",
      "epoch:412\tacc:0.9705882352941176 \t loss:0.02709551565349102\n",
      "epoch:413\tacc:0.9647058823529412 \t loss:0.03361775912344456\n",
      "epoch:414\tacc:0.9705882352941176 \t loss:0.026915529370307924\n",
      "epoch:415\tacc:0.9705882352941176 \t loss:0.025418072193861007\n",
      "epoch:416\tacc:0.9705882352941176 \t loss:0.03369344770908356\n",
      "epoch:417\tacc:0.9705882352941176 \t loss:0.02709670066833496\n",
      "epoch:418\tacc:0.9705882352941176 \t loss:0.025614736974239348\n",
      "epoch:419\tacc:0.9764705882352941 \t loss:0.025412264838814737\n",
      "epoch:420\tacc:0.9588235294117647 \t loss:0.0472266610711813\n",
      "epoch:421\tacc:0.9117647058823529 \t loss:0.09357457682490349\n",
      "epoch:422\tacc:0.8352941176470589 \t loss:0.14175470918416977\n",
      "epoch:423\tacc:0.8588235294117647 \t loss:0.1607438400387764\n",
      "epoch:424\tacc:0.9117647058823529 \t loss:0.07619669511914254\n",
      "epoch:425\tacc:0.9058823529411765 \t loss:0.07980313301086425\n",
      "epoch:426\tacc:0.9411764705882353 \t loss:0.05554809048771858\n",
      "epoch:427\tacc:0.9529411764705882 \t loss:0.05229661241173744\n",
      "epoch:428\tacc:0.9470588235294117 \t loss:0.049641850590705874\n",
      "epoch:429\tacc:0.9588235294117647 \t loss:0.039680081978440286\n",
      "epoch:430\tacc:0.9647058823529412 \t loss:0.039373311400413516\n",
      "epoch:431\tacc:0.9411764705882353 \t loss:0.05233155451714992\n",
      "epoch:432\tacc:0.9529411764705882 \t loss:0.03738696277141571\n",
      "epoch:433\tacc:0.9705882352941176 \t loss:0.028166284784674644\n",
      "epoch:434\tacc:0.9705882352941176 \t loss:0.02631322517991066\n",
      "epoch:435\tacc:0.9705882352941176 \t loss:0.026891298219561578\n",
      "epoch:436\tacc:0.9705882352941176 \t loss:0.026951644197106363\n",
      "epoch:437\tacc:0.9705882352941176 \t loss:0.02598637044429779\n",
      "epoch:438\tacc:0.9764705882352941 \t loss:0.025428524985909463\n",
      "epoch:439\tacc:0.9705882352941176 \t loss:0.024526584893465042\n",
      "epoch:440\tacc:0.9823529411764705 \t loss:0.0240458644926548\n",
      "epoch:441\tacc:0.9647058823529412 \t loss:0.02661769762635231\n",
      "epoch:442\tacc:0.9470588235294117 \t loss:0.04966341853141785\n",
      "epoch:443\tacc:0.9647058823529412 \t loss:0.03184567168354988\n",
      "epoch:444\tacc:0.9647058823529412 \t loss:0.03211159035563469\n",
      "epoch:445\tacc:0.9705882352941176 \t loss:0.026198507100343705\n",
      "epoch:446\tacc:0.9705882352941176 \t loss:0.02464805692434311\n",
      "epoch:447\tacc:0.9705882352941176 \t loss:0.024123671650886535\n",
      "epoch:448\tacc:0.9764705882352941 \t loss:0.02400803230702877\n",
      "epoch:449\tacc:0.9823529411764705 \t loss:0.025740677118301393\n",
      "epoch:450\tacc:0.9882352941176471 \t loss:0.025886324420571327\n",
      "epoch:451\tacc:0.9823529411764705 \t loss:0.023884427547454835\n",
      "epoch:452\tacc:0.9823529411764705 \t loss:0.025646782666444778\n",
      "epoch:453\tacc:0.9764705882352941 \t loss:0.03746680803596973\n",
      "epoch:454\tacc:0.9705882352941176 \t loss:0.03190774619579315\n",
      "epoch:455\tacc:0.9705882352941176 \t loss:0.02582291029393673\n",
      "epoch:456\tacc:0.9764705882352941 \t loss:0.024421432241797447\n",
      "epoch:457\tacc:0.9705882352941176 \t loss:0.024082424491643904\n",
      "epoch:458\tacc:0.9764705882352941 \t loss:0.024163032323122023\n",
      "epoch:459\tacc:0.9764705882352941 \t loss:0.023226508498191835\n",
      "epoch:460\tacc:0.9882352941176471 \t loss:0.022676629573106767\n",
      "epoch:461\tacc:1.0 \t loss:0.022445975244045256\n",
      "epoch:462\tacc:1.0 \t loss:0.022276347503066063\n",
      "epoch:463\tacc:0.9882352941176471 \t loss:0.022164424508810045\n",
      "epoch:464\tacc:0.9882352941176471 \t loss:0.022078202292323112\n",
      "epoch:465\tacc:0.9941176470588236 \t loss:0.021980033442378043\n",
      "epoch:466\tacc:1.0 \t loss:0.021904104202985764\n",
      "epoch:467\tacc:1.0 \t loss:0.021832355111837388\n",
      "epoch:468\tacc:1.0 \t loss:0.021763480827212335\n",
      "epoch:469\tacc:1.0 \t loss:0.02169712781906128\n",
      "epoch:470\tacc:1.0 \t loss:0.021633092686533927\n",
      "epoch:471\tacc:1.0 \t loss:0.02157095745205879\n",
      "epoch:472\tacc:1.0 \t loss:0.02151056155562401\n",
      "epoch:473\tacc:1.0 \t loss:0.021451403945684434\n",
      "epoch:474\tacc:1.0 \t loss:0.021393703669309615\n",
      "epoch:475\tacc:1.0 \t loss:0.02133711576461792\n",
      "epoch:476\tacc:1.0 \t loss:0.02128169424831867\n",
      "epoch:477\tacc:1.0 \t loss:0.02122737988829613\n",
      "epoch:478\tacc:1.0 \t loss:0.02117411158978939\n",
      "epoch:479\tacc:1.0 \t loss:0.021121798455715178\n",
      "epoch:480\tacc:1.0 \t loss:0.021070464327931405\n",
      "epoch:481\tacc:1.0 \t loss:0.021020043268799783\n",
      "epoch:482\tacc:1.0 \t loss:0.020970509946346284\n",
      "epoch:483\tacc:1.0 \t loss:0.02092184014618397\n",
      "epoch:484\tacc:1.0 \t loss:0.020874029397964476\n",
      "epoch:485\tacc:1.0 \t loss:0.02082701213657856\n",
      "epoch:486\tacc:1.0 \t loss:0.02078082859516144\n",
      "epoch:487\tacc:1.0 \t loss:0.020735347643494606\n",
      "epoch:488\tacc:1.0 \t loss:0.020690665394067765\n",
      "epoch:489\tacc:1.0 \t loss:0.020646846294403075\n",
      "epoch:490\tacc:1.0 \t loss:0.020610380917787552\n",
      "epoch:491\tacc:1.0 \t loss:0.020766449719667436\n",
      "epoch:492\tacc:0.9823529411764705 \t loss:0.024835291877388956\n",
      "epoch:493\tacc:0.9470588235294117 \t loss:0.0670325979590416\n",
      "epoch:494\tacc:0.888235294117647 \t loss:0.1612204447388649\n",
      "epoch:495\tacc:0.8294117647058824 \t loss:0.16031257808208466\n",
      "epoch:496\tacc:0.8647058823529412 \t loss:0.12125667780637742\n",
      "epoch:497\tacc:0.9117647058823529 \t loss:0.06588388308882713\n",
      "epoch:498\tacc:0.9411764705882353 \t loss:0.0706355519592762\n",
      "epoch:499\tacc:0.8764705882352941 \t loss:0.11232966408133507\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▂▃▃▄▅▆▆▇▇▇▇███████▇▇▇█▇█████▇█████████▇</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.87647</td></tr><tr><td>train_loss</td><td>0.11233</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scintillating-tiger-14</strong> at: <a href=\"https://wandb.ai/andompesta/astrazeneca/runs/smwvwuk6\" target=\"_blank\">https://wandb.ai/andompesta/astrazeneca/runs/smwvwuk6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230202_164332-smwvwuk6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for trial in trials:\n",
    "    with wandb.init(**trial) as exp:\n",
    "        dev_dataset = WikiDataset(\n",
    "            exp.config.dataset_base_path,\n",
    "            exp.config.dataset_name,\n",
    "            exp.config.vocab_path,\n",
    "        )\n",
    "        exp.config[\"vocab_size\"] = len(dev_dataset.entity_2_id.data)\n",
    "\n",
    "        dev_dl = DataLoader(\n",
    "            dev_dataset,\n",
    "            batch_size=exp.config.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # as it is a single batch experiment\n",
    "        batch_data = next(iter(dev_dl))\n",
    "        dev_dl = [batch_data] * exp.config.steps_per_epoch\n",
    "\n",
    "        # create model\n",
    "        model = GraphSeq(\n",
    "            emb_dim=exp.config.emb_dim,\n",
    "            vocab_size=exp.config.vocab_size,\n",
    "            pad_idx=exp.config.pad_idx,\n",
    "            graph_conv_layers=exp.config.graph_conv_layers,\n",
    "            rnn_decoder_layers=exp.config.rnn_decoder_layers,\n",
    "            rnn_dropout=exp.config.rnn_dropout,\n",
    "        )\n",
    "\n",
    "        # create optimizer\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=exp.config.learning_rate,\n",
    "        )\n",
    "\n",
    "        # setup for training\n",
    "        device = exp.config.device\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        model = model.to(device)\n",
    "\n",
    "        for epoch in range(exp.config.epochs):\n",
    "\n",
    "            metrics = train_fn(\n",
    "                model=model,\n",
    "                dataloader=dev_dl,\n",
    "                optimizer=optimizer,\n",
    "                steps_per_epoch=exp.config.steps_per_epoch,\n",
    "                device=device,\n",
    "                gradient_accumulation_steps=exp.config.accumulation_steps,\n",
    "                pad_idx=exp.config.pad_idx,\n",
    "                max_grad_norm=exp.config.max_grad_norm,\n",
    "            )\n",
    "\n",
    "            print(\"epoch:{epoch}\\tacc:{acc} \\t loss:{loss}\".format(\n",
    "                epoch=epoch,\n",
    "                acc=metrics[\"train_accuracy\"],\n",
    "                loss=metrics[\"train_loss\"],\n",
    "            ))\n",
    "            exp.log(metrics, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7563ebfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    4,    40,     6,    34,     3,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2],\n",
       "         [    4,    17,    34,     6,    35,   227,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2],\n",
       "         [    4,    34,     6,    35,   681,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2],\n",
       "         [    4,    42,     6,    35,   227,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2],\n",
       "         [    4,    42,     6,     6,   681,     5,     5, 14416,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2]], device='cuda:0'),\n",
       " tensor([[    4,    40,     6,    34,     3,     2,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    4,    17,    34,     6,    35,   227,     2,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    4,    34,     6,    35,   681,     2,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    4,    42,     6,    35,  1061,     2,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    4,    42,     6,    40,  5271,     5,    37, 14416,     2,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]], device='cuda:0'),\n",
       " tensor([[    1,     4,    40,     6,    34,     3,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,     4,    17,    34,     6,    35,   227,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,     4,    34,     6,    35,   681,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,     4,    42,     6,    35,  1061,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,     4,    42,     6,    40,  5271,     5,    37, 14416,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(\n",
    "    batch_data.x,\n",
    "    batch_data.src_seq,\n",
    "    batch_data.edge_index,\n",
    "    batch_data.bw_edge_index,\n",
    "    batch_data.batch,\n",
    ")\n",
    "\n",
    "preds = logits.softmax(-1).argmax(-1)\n",
    "preds, batch_data.trg_seq, batch_data.src_seq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfbe21d3",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "As we are able to overfit a single batch, we can move to the next step\n",
    "\n",
    "*Goal*: train and eval on full dataset with simple model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7cd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"graph-seq train and eval\"\n",
    "\n",
    "trials = [\n",
    "    # trial setup\n",
    "    dict(\n",
    "        job_type=\"train\",\n",
    "        project=project,\n",
    "        group=experiment_name,\n",
    "        notes=\n",
    "        \"test training and validation pipeline on the entire dataset with a simple model\",\n",
    "        config=dict(\n",
    "            dataset_base_path=\"data/wiki\",\n",
    "            train_dataset_name=\"train\",\n",
    "            dev_dataset_name=\"dev\",\n",
    "            vocab_path=\"data/wiki/entity_2_id.bin\",\n",
    "            batch_size=64,\n",
    "            learning_rate=0.003,\n",
    "            device=\"cuda\",\n",
    "            accumulation_steps=1,\n",
    "            max_grad_norm=20.,\n",
    "            epochs=10,\n",
    "            pad_idx=0,\n",
    "            emb_dim=60,\n",
    "            graph_conv_layers=3,\n",
    "            rnn_layers=2,\n",
    "            rnn_dropout=0.5,\n",
    "        ),\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02d6623a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ando_cavallari/astra-zeneca/wandb/run-20230202_164907-0hbwi35f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/andompesta/astrazeneca/runs/0hbwi35f\" target=\"_blank\">beaming-envelope-15</a></strong> to <a href=\"https://wandb.ai/andompesta/astrazeneca\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/andompesta/astrazeneca\" target=\"_blank\">https://wandb.ai/andompesta/astrazeneca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/andompesta/astrazeneca/runs/0hbwi35f\" target=\"_blank\">https://wandb.ai/andompesta/astrazeneca/runs/0hbwi35f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 50\n",
      "batch : 100\n",
      "batch : 150\n",
      "batch : 200\n",
      "batch : 250\n",
      "batch : 300\n",
      "batch : 350\n",
      "batch : 400\n",
      "batch : 450\n",
      "batch : 500\n",
      "batch : 550\n",
      "batch : 600\n",
      "batch : 650\n",
      "batch : 700\n",
      "batch : 750\n",
      "batch : 800\n",
      "batch : 850\n",
      "epoch:0\tacc:0.46205942798218075 \t loss:1.4920014627404705\n",
      "eval batch : 50\n",
      "eval batch : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'eval_loss': 0.9521422697739168, 'eval_accuracy': 0.5877499919745754, 'eval_blue_score': 0.10205085511420839}\n",
      "\n",
      "batch : 50\n",
      "batch : 100\n",
      "batch : 150\n",
      "batch : 200\n",
      "batch : 250\n",
      "batch : 300\n",
      "batch : 350\n",
      "batch : 400\n",
      "batch : 450\n",
      "batch : 500\n",
      "batch : 550\n",
      "batch : 600\n",
      "batch : 650\n",
      "batch : 700\n",
      "batch : 750\n",
      "batch : 800\n",
      "batch : 850\n",
      "epoch:1\tacc:0.5806792801952503 \t loss:0.9575620880191903\n",
      "eval batch : 50\n",
      "eval batch : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'eval_loss': 0.7148003117604689, 'eval_accuracy': 0.6526435748451093, 'eval_blue_score': 0.1839454358009507}\n",
      "\n",
      "batch : 50\n",
      "batch : 100\n",
      "batch : 150\n",
      "batch : 200\n",
      "batch : 250\n",
      "batch : 300\n",
      "batch : 350\n",
      "batch : 400\n",
      "batch : 450\n",
      "batch : 500\n",
      "batch : 550\n",
      "batch : 600\n",
      "batch : 650\n",
      "batch : 700\n",
      "batch : 750\n",
      "batch : 800\n",
      "batch : 850\n",
      "epoch:2\tacc:0.6296894161524803 \t loss:0.7839474743327272\n",
      "eval batch : 50\n",
      "eval batch : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 {'eval_loss': 0.5599505885532408, 'eval_accuracy': 0.7111810214760361, 'eval_blue_score': 0.2717121205156288}\n",
      "\n",
      "batch : 50\n",
      "batch : 100\n",
      "batch : 150\n",
      "batch : 200\n",
      "batch : 250\n",
      "batch : 300\n",
      "batch : 350\n",
      "batch : 400\n",
      "batch : 450\n",
      "batch : 500\n",
      "batch : 550\n",
      "batch : 600\n",
      "batch : 650\n",
      "batch : 700\n",
      "batch : 750\n",
      "batch : 800\n",
      "batch : 850\n",
      "epoch:3\tacc:0.6715583642728589 \t loss:0.6657895584517792\n",
      "eval batch : 50\n",
      "eval batch : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 {'eval_loss': 0.4461886298024293, 'eval_accuracy': 0.7669416712144073, 'eval_blue_score': 0.38070674969269325}\n",
      "\n",
      "batch : 50\n",
      "batch : 100\n",
      "batch : 150\n",
      "batch : 200\n",
      "batch : 250\n",
      "batch : 300\n",
      "batch : 350\n",
      "batch : 400\n",
      "batch : 450\n",
      "batch : 500\n",
      "batch : 550\n",
      "batch : 600\n",
      "batch : 650\n",
      "batch : 700\n",
      "batch : 750\n",
      "batch : 800\n",
      "batch : 850\n",
      "epoch:4\tacc:0.703831040074014 \t loss:0.5802082945418277\n",
      "eval batch : 50\n",
      "eval batch : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 {'eval_loss': 0.3654428055566369, 'eval_accuracy': 0.8064588616737826, 'eval_blue_score': 0.4638796137146781}\n",
      "\n",
      "batch : 50\n",
      "batch : 100\n",
      "batch : 150\n",
      "batch : 200\n",
      "batch : 250\n",
      "batch : 300\n",
      "batch : 350\n",
      "batch : 400\n",
      "batch : 450\n",
      "batch : 500\n",
      "batch : 550\n",
      "batch : 600\n",
      "batch : 650\n",
      "batch : 700\n",
      "batch : 750\n",
      "batch : 800\n",
      "batch : 850\n",
      "epoch:5\tacc:0.7286444994301987 \t loss:0.5173888485223291\n",
      "eval batch : 50\n",
      "eval batch : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 {'eval_loss': 0.30989974037264334, 'eval_accuracy': 0.8331835254084942, 'eval_blue_score': 0.5333742155627581}\n",
      "\n",
      "batch : 50\n",
      "batch : 100\n",
      "batch : 150\n",
      "batch : 200\n",
      "batch : 250\n",
      "batch : 300\n",
      "batch : 350\n",
      "batch : 400\n",
      "batch : 450\n",
      "batch : 500\n",
      "batch : 550\n",
      "batch : 600\n",
      "batch : 650\n",
      "batch : 700\n",
      "batch : 750\n",
      "batch : 800\n",
      "batch : 850\n",
      "epoch:6\tacc:0.7467239439402299 \t loss:0.4691592994633381\n",
      "eval batch : 50\n",
      "eval batch : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 {'eval_loss': 0.25836929357187316, 'eval_accuracy': 0.858688324612372, 'eval_blue_score': 0.6103597191551461}\n",
      "\n",
      "batch : 50\n",
      "batch : 100\n",
      "batch : 150\n",
      "batch : 200\n",
      "batch : 250\n",
      "batch : 300\n",
      "batch : 350\n",
      "batch : 400\n",
      "batch : 450\n",
      "batch : 500\n",
      "batch : 550\n",
      "batch : 600\n",
      "batch : 650\n",
      "batch : 700\n",
      "batch : 750\n",
      "batch : 800\n",
      "batch : 850\n",
      "epoch:7\tacc:0.7627602955738609 \t loss:0.42965020543803695\n",
      "eval batch : 50\n",
      "eval batch : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 {'eval_loss': 0.22046721980653025, 'eval_accuracy': 0.8805335302237488, 'eval_blue_score': 0.6779827867448665}\n",
      "\n",
      "batch : 50\n",
      "batch : 100\n",
      "batch : 150\n",
      "batch : 200\n",
      "batch : 250\n",
      "batch : 300\n",
      "batch : 350\n",
      "batch : 400\n",
      "batch : 450\n",
      "batch : 500\n",
      "batch : 550\n",
      "batch : 600\n",
      "batch : 650\n",
      "batch : 700\n",
      "batch : 750\n",
      "batch : 800\n",
      "batch : 850\n",
      "epoch:8\tacc:0.7749248898344083 \t loss:0.4011964034141125\n",
      "eval batch : 50\n",
      "eval batch : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 {'eval_loss': 0.19530869359997186, 'eval_accuracy': 0.8921222432666688, 'eval_blue_score': 0.7122706705837628}\n",
      "\n",
      "batch : 50\n",
      "batch : 100\n",
      "batch : 150\n",
      "batch : 200\n",
      "batch : 250\n",
      "batch : 300\n",
      "batch : 350\n",
      "batch : 400\n",
      "batch : 450\n",
      "batch : 500\n",
      "batch : 550\n",
      "batch : 600\n",
      "batch : 650\n",
      "batch : 700\n",
      "batch : 750\n",
      "batch : 800\n",
      "batch : 850\n",
      "epoch:9\tacc:0.7855499365147438 \t loss:0.37670996739496304\n",
      "eval batch : 50\n",
      "eval batch : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/geometric/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 {'eval_loss': 0.17914718208890973, 'eval_accuracy': 0.9009020577188533, 'eval_blue_score': 0.7389551826714676}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>▁▂▄▅▆▆▇███</td></tr><tr><td>eval_blue_score</td><td>▁▂▃▄▅▆▇▇██</td></tr><tr><td>eval_loss</td><td>█▆▄▃▃▂▂▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▆▇▇███</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_accuracy</td><td>0.9009</td></tr><tr><td>eval_blue_score</td><td>0.73896</td></tr><tr><td>eval_loss</td><td>0.17915</td></tr><tr><td>train_accuracy</td><td>0.78555</td></tr><tr><td>train_loss</td><td>0.37671</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">beaming-envelope-15</strong> at: <a href=\"https://wandb.ai/andompesta/astrazeneca/runs/0hbwi35f\" target=\"_blank\">https://wandb.ai/andompesta/astrazeneca/runs/0hbwi35f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230202_164907-0hbwi35f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for trial in trials:\n",
    "    with wandb.init(**trial) as exp:\n",
    "        train_dataset = WikiDataset(\n",
    "            exp.config.dataset_base_path,\n",
    "            exp.config.train_dataset_name,\n",
    "            exp.config.vocab_path,\n",
    "        )\n",
    "        train_dl = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=exp.config.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        exp.config.steps_per_epoch = len(train_dl)\n",
    "        exp.config.vocab_size = len(dev_dataset.entity_2_id.data)\n",
    "\n",
    "        dev_dataset = WikiDataset(\n",
    "            exp.config.dataset_base_path,\n",
    "            exp.config.dev_dataset_name,\n",
    "            exp.config.vocab_path,\n",
    "        )\n",
    "        dev_dl = DataLoader(\n",
    "            dev_dataset,\n",
    "            batch_size=exp.config.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # create model\n",
    "        model = GraphSeq(\n",
    "            emb_dim=exp.config.emb_dim,\n",
    "            vocab_size=exp.config.vocab_size,\n",
    "            pad_idx=exp.config.pad_idx,\n",
    "            graph_conv_layers=exp.config.graph_conv_layers,\n",
    "            rnn_decoder_layers=exp.config.rnn_layers,\n",
    "            rnn_dropout=exp.config.rnn_dropout,\n",
    "        )\n",
    "\n",
    "        # create optimizer\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=exp.config.learning_rate,\n",
    "        )\n",
    "\n",
    "        device = exp.config.device\n",
    "        # setup for training\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        model = model.to(device)\n",
    "\n",
    "        for epoch in range(exp.config.epochs):\n",
    "\n",
    "            metrics = train_fn(\n",
    "                model=model,\n",
    "                dataloader=dev_dl,\n",
    "                optimizer=optimizer,\n",
    "                steps_per_epoch=exp.config.steps_per_epoch,\n",
    "                device=device,\n",
    "                gradient_accumulation_steps=exp.config.accumulation_steps,\n",
    "                pad_idx=exp.config.pad_idx,\n",
    "                max_grad_norm=exp.config.max_grad_norm,\n",
    "            )\n",
    "\n",
    "            print(\"epoch:{epoch}\\tacc:{acc} \\t loss:{loss}\".format(\n",
    "                epoch=epoch,\n",
    "                acc=metrics[\"train_accuracy\"],\n",
    "                loss=metrics[\"train_loss\"],\n",
    "            ))\n",
    "            exp.log(metrics, step=epoch)\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                # eval every 1 epochs\n",
    "                is_best = False\n",
    "                scores = eval_fn(\n",
    "                    model=model,\n",
    "                    dataloader=dev_dl,\n",
    "                    device=device,\n",
    "                )\n",
    "\n",
    "                print(epoch, scores)\n",
    "                print()\n",
    "                exp.log(scores, step=epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "1debeed81619785a8d6b1d6ab4f3be2c289b30083acd6f537a7446573ac7344f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
